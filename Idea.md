# CodeWiki Generator - Core Idea & Philosophy

## The Central Insight

**Code tells you what. Documentation tells you why. History tells you how.**

Most documentation systems fail because they're:
- Written once, never updated
- Separate from the code they describe
- Created after the fact, missing context
- Expensive to maintain

But what if documentation could **grow organically** alongside code? What if understanding a codebase was like watching a time-lapse of its construction, seeing not just the final building but every brick laid in order?

## The Idea

A system that walks through git history chronologically and builds a living wiki that captures:
- **Concepts**: The mental models and abstractions
- **Components**: The actual code structures
- **Relationships**: How pieces connect
- **Evolution**: How understanding deepened over time

The wiki isn't just a static snapshot—it's **archaeological documentation**. You can see when concepts were introduced, when they changed, when they were refined. The documentation carries the story of the codebase's intellectual development.

## The Philosophy

### 1. Documentation as Emergent Property

Good documentation emerges from understanding, not from exhaustive cataloging. The system starts simple:
- First pass: "This file handles authentication"
- Later: "This is OAuth2 with refresh token rotation"
- Even later: "This implements RFC 6749 with custom security extensions"

Understanding deepens naturally as the system processes more commits. Just like a human learning a codebase.

### 2. Organic Growth, Not Comprehensive Coverage

Not everything deserves documentation. A wiki page should exist because it's **useful**, not because it's **complete**. 

Important code gets referenced repeatedly → Gets detailed documentation  
Peripheral utilities mentioned once → Gets a brief note  
Boring config files → Might be ignored entirely

The system discovers what matters by seeing what's significant in commits and what other code depends on.

### 3. Self-Limiting Complexity

Wikis naturally stay manageable because verbosity is a bug, not a feature. If a page grows too long, it becomes less useful, signaling the need to split it. If pages have redundant content, consolidation improves quality.

The medium itself enforces good practices. You can't have a 10,000-word wiki page without noticing something's wrong.

### 4. The Documentation as Truth Test

Here's the meta-insight: **If you build a documentation system that documents itself poorly, the system is broken.**

This project practices radical self-reference:
- The system documents codebases by analyzing git history
- The system's own git history is its test case
- If you can't understand this codebase from its own generated wiki, the documentation system has failed
- Fix the system until its self-documentation is excellent

This creates a virtuous feedback loop: improving the documentation system improves its ability to document itself, which reveals new ways to improve the system.

## The Vision

### For Developers

Imagine opening an unfamiliar codebase and asking:
- "How does authentication work?" → Wiki has a clear page
- "Where are tests configured?" → Operational guide exists
- "Why was this approach chosen?" → History shows the decision point

The wiki becomes your **external brain** for the codebase. It's always current because it's regenerated from history. It's always consistent because it's generated by the same AI understanding the patterns.

### For AI Coding Agents

Current problem: AI agents get a massive context dump of code and struggle to find relevant information.

With CodeWiki:
- Agent queries: "How do I run tests?"
- MCP server returns the exact wiki page documenting test setup
- Agent has perfect context, not overwhelming context

The wiki acts as a **compressed, indexed, structured knowledge base**—optimized for retrieval, not raw code dumping.

Better: when the agent encounters something undocumented, it requests documentation, the system adds it to the queue, and future agents benefit.

### The Feedback Loop

```
Developer commits code
    ↓
System generates documentation
    ↓
Developer reads documentation (or AI agent queries it)
    ↓
Gaps or confusion identified
    ↓
Documentation requests queued
    ↓
System fills gaps in next run
    ↓
Documentation improves
```

The system **learns what documentation is needed** by observing what gets requested.

## Unique Aspects

### Time as a Feature, Not a Bug

Most documentation tries to hide history ("Here's how it works now"). This system **embraces history** ("Here's how understanding evolved").

Seeing that "UserManager" became "AuthenticationService" at commit 147 tells you something valuable about the architecture's maturation.

### Human + AI Collaboration

The system isn't fully autonomous or fully manual:
- AI handles bulk processing (generates docs from commits)
- Humans handle judgment calls (this is important, that isn't)
- Meta-analysis agent proposes improvements
- Humans approve or reject
- Humans can edit wiki pages directly
- AI learns from human edits

It's a **pair programming model** for documentation.

### Documentation That Documents Its Own Quality

Normal documentation: silent about its coverage or accuracy  
CodeWiki: knows which pages are well-linked (probably important), which are orphaned (maybe obsolete), which get frequently requested (need improvement)

The **metadata about documentation** is itself valuable information.

## Key Principles

### Start Minimal, Grow Organically
Don't try to document everything on day one. Let importance reveal itself through repeated references and usage patterns.

### Embrace Imperfection
Early documentation will be rough. That's fine—it improves with each pass. Stale is worse than imperfect.

### Trust the Medium
Wikis have survived decades because the format works. Internal linking, simple markup, human readability—don't fight the medium.

### Make It Useful First, Complete Later
A single excellent page about authentication is more valuable than 50 mediocre pages cataloging every function.

### The System Should Love Being Used On Itself
If building this system makes you avoid using it on itself, something is wrong. The best validation is dogfooding.

## Design Philosophy

### For Humans
- Dashboard shows exactly what's happening (no black box)
- Manual stepping lets you verify before trusting
- Edit pages directly when AI gets it wrong
- Controls are simple: start, pause, step

### For AI Agents
- Small, focused tasks (analyze one file, write one page)
- Structured inputs and outputs (JSON for agent-to-agent communication)
- Clear context boundaries (max 3 related pages, truncated diffs)
- Feedback mechanisms (request queue for missing info)

### For the System Itself
- Test-driven development proves it works
- Self-documentation proves it's useful  
- Git commits provide audit trail
- Modularity allows improvements without rewrites

## What This Isn't

**Not a code search engine**: Wiki is curated understanding, not indexed raw code

**Not a replacement for human docs**: Critical guides still need human writing

**Not comprehensive**: Deliberately selective about what to document

**Not real-time**: Documentation is generated, not live-updated

**Not a silver bullet**: Good architecture documentation still requires architectural thinking

## What This Is

**A time machine for understanding**: See how code evolved, not just its current state

**A knowledge compressor**: Turns 100,000 lines of code into 50 pages of insight

**A collaboration tool**: Human judgment + AI processing = better than either alone

**A self-improving system**: Quality feedback improves future documentation

**An experiment in meta-software**: Can software that documents software document itself well?

## Success Looks Like

You open this system's codebase for the first time. You read the generated wiki for 15 minutes. You now understand:
- The overall architecture
- Where each major component lives
- How to run and test it
- Why key decisions were made

You start using Claude Code with the MCP server. It asks intelligent questions because it has perfect context from the wiki. You make a change. The wiki updates. The AI agent uses the updated wiki. The loop closes.

A new developer joins your team. They read the wiki for an hour. They're productive. Not because the wiki is comprehensive, but because it answers the right questions.

## The Meta-Question

Here's the philosophical heart of this project:

**Can a system that generates understanding generate understanding of itself?**

If yes, we've created something genuinely useful—a documentation tool that proves its value by documenting itself excellently.

If no, we learn exactly where AI-generated documentation fails by seeing it fail on the ideal test case (itself).

Either outcome teaches us something profound about documentation, understanding, and AI capabilities.

## Getting Started

For humans: Read the spec, use the implementation guide, build it, run it on itself, see if the generated wiki is good.

For AI agents: The implementation guide is your roadmap. Follow test-driven development. Use the system on itself frequently. If the self-generated wiki is unclear, you're doing it wrong—fix the documentation system before continuing.

The true test isn't whether you can build it. The test is whether what you build can adequately explain itself.

---

**Remember**: This project is a loop that closes on itself. The better the documentation system, the better it documents itself, revealing more ways to improve. That recursive improvement is the entire point.

**Core Belief**: Good documentation is like good teaching—it meets learners where they are and guides them to understanding. This system learns to teach by teaching itself.

**Ultimate Goal**: Make understanding codebases feel less like archaeology and more like having a knowledgeable guide who was there for the entire journey.

---

*This is not just a tool. It's an exploration of whether machines can help us understand complexity—starting with helping us understand themselves.*
